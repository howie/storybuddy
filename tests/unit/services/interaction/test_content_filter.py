"""Unit tests for Content Filter service.

T048 [P] [US2] Unit test for content filter.
Tests the content filtering for child-safe AI responses.
"""

import pytest

# These imports will fail until the service is implemented
from src.services.interaction.content_filter import (
    ContentCategory,
    ContentFilter,
    ContentFilterConfig,
    FilterResult,
)


class TestContentFilterConfig:
    """Tests for Content Filter configuration."""

    def test_default_config_values(self):
        """Default config should be maximally restrictive for child safety."""
        config = ContentFilterConfig()
        assert config.strictness_level == "maximum"
        assert config.enable_profanity_filter is True
        assert config.enable_violence_filter is True
        assert config.enable_adult_content_filter is True

    def test_custom_blocked_phrases(self):
        """Should allow custom blocked phrases."""
        config = ContentFilterConfig(custom_blocked_phrases=["å£è©±1", "å£è©±2"])
        assert "å£è©±1" in config.custom_blocked_phrases

    def test_custom_allowed_phrases(self):
        """Should allow custom allowed phrases for story context."""
        config = ContentFilterConfig(
            custom_allowed_phrases=["å¤§é‡ç‹¼"]  # Allowed in story context
        )
        assert "å¤§é‡ç‹¼" in config.custom_allowed_phrases


class TestFilterResult:
    """Tests for filter result model."""

    def test_create_safe_result(self):
        """Should create result for safe content."""
        result = FilterResult(
            is_safe=True,
            original_text="å°å…”å­åœ¨åƒè˜¿è””",
            filtered_text="å°å…”å­åœ¨åƒè˜¿è””",
            categories_detected=[],
        )
        assert result.is_safe is True
        assert result.original_text == result.filtered_text

    def test_create_unsafe_result(self):
        """Should create result for unsafe content with detected categories."""
        result = FilterResult(
            is_safe=False,
            original_text="ä¸å®‰å…¨çš„å…§å®¹",
            filtered_text=None,
            categories_detected=[ContentCategory.INAPPROPRIATE_LANGUAGE],
            reason="Contains inappropriate language",
        )
        assert result.is_safe is False
        assert ContentCategory.INAPPROPRIATE_LANGUAGE in result.categories_detected

    def test_create_modified_result(self):
        """Should create result for content that was modified."""
        result = FilterResult(
            is_safe=True,
            original_text="æœ‰äº›å•é¡Œçš„å…§å®¹",
            filtered_text="ä¿®æ”¹å¾Œçš„å®‰å…¨å…§å®¹",
            was_modified=True,
            categories_detected=[ContentCategory.OFF_TOPIC],
        )
        assert result.was_modified is True
        assert result.original_text != result.filtered_text


class TestContentFilter:
    """Tests for Content Filter service."""

    @pytest.fixture
    def content_filter(self):
        """Create a Content Filter instance."""
        return ContentFilter()

    def test_create_filter(self, content_filter):
        """Should create filter with default configuration."""
        assert content_filter is not None
        assert content_filter.config.strictness_level == "maximum"

    def test_safe_content_passes(self, content_filter):
        """Should allow safe, story-related content."""
        result = content_filter.filter("å°å…”å­åœ¨æ£®æ—è£¡è·³ä¾†è·³å»ï¼Œå¥½é–‹å¿ƒå•Šï¼")
        assert result.is_safe is True
        assert result.was_modified is False

    def test_basic_greeting_passes(self, content_filter):
        """Should allow basic greetings."""
        result = content_filter.filter("ä½ å¥½ï¼")
        assert result.is_safe is True

    def test_story_related_question_passes(self, content_filter):
        """Should allow story-related questions."""
        result = content_filter.filter("å°å…”å­æœƒä¸æœƒé‡åˆ°å¤§é‡ç‹¼ï¼Ÿ")
        assert result.is_safe is True

    def test_educational_content_passes(self, content_filter):
        """Should allow educational content."""
        result = content_filter.filter("ç‚ºä»€éº¼å…”å­å–œæ­¡åƒç´…è˜¿è””ï¼Ÿ")
        assert result.is_safe is True


class TestContentFilterProfanity:
    """Tests for profanity filtering."""

    @pytest.fixture
    def content_filter(self):
        """Create a Content Filter instance."""
        return ContentFilter()

    def test_blocks_profanity(self, content_filter):
        """Should block profane language."""
        result = content_filter.filter("é€™æ˜¯é«’è©±å…§å®¹")
        # Filter should detect and handle profanity
        assert (
            ContentCategory.INAPPROPRIATE_LANGUAGE in result.categories_detected or result.is_safe
        )

    def test_detects_masked_profanity(self, content_filter):
        """Should detect masked profanity attempts."""
        # Various masking techniques
        result = content_filter.filter("é€™æ˜¯*#%&å…§å®¹")
        # Should at least flag for review or filter
        assert result is not None


class TestContentFilterViolence:
    """Tests for violence content filtering."""

    @pytest.fixture
    def content_filter(self):
        """Create a Content Filter instance."""
        return ContentFilter()

    def test_blocks_graphic_violence(self, content_filter):
        """Should block graphic violence descriptions."""
        result = content_filter.filter("æ®ºæ‰æ‰€æœ‰äºº")
        assert result.is_safe is False or ContentCategory.VIOLENCE in result.categories_detected

    def test_allows_story_conflict(self, content_filter):
        """Should allow mild story conflicts (e.g., wolf chasing rabbit)."""
        result = content_filter.filter("å¤§é‡ç‹¼è¿½è‘—å°å…”å­è·‘")
        assert result.is_safe is True

    def test_context_aware_violence_filtering(self, content_filter):
        """Should consider context when filtering violence."""
        # Story context should allow certain narrative elements
        result = content_filter.filter(
            "å°å…”å­å‹‡æ•¢åœ°èº²éäº†å¤§é‡ç‹¼", context={"story_title": "å°å…”å­å†’éšªè¨˜"}
        )
        assert result.is_safe is True


class TestContentFilterAdultContent:
    """Tests for adult content filtering."""

    @pytest.fixture
    def content_filter(self):
        """Create a Content Filter instance."""
        return ContentFilter()

    def test_blocks_adult_themes(self, content_filter):
        """Should block adult themes."""
        result = content_filter.filter("æˆäººå…§å®¹")
        # Should be filtered
        assert (
            result.is_safe is False or ContentCategory.ADULT_CONTENT in result.categories_detected
        )

    def test_blocks_romantic_content(self, content_filter):
        """Should block romantic content inappropriate for children."""
        result = content_filter.filter("è¦ªå¯†è¡Œç‚ºæè¿°")
        assert result.is_safe is False or len(result.categories_detected) > 0


class TestContentFilterPersonalInfo:
    """Tests for personal information filtering."""

    @pytest.fixture
    def content_filter(self):
        """Create a Content Filter instance."""
        return ContentFilter()

    def test_detects_phone_number_requests(self, content_filter):
        """Should detect phone number requests."""
        result = content_filter.filter("å‘Šè¨´æˆ‘ä½ çš„é›»è©±è™Ÿç¢¼")
        assert ContentCategory.PERSONAL_INFO_REQUEST in result.categories_detected

    def test_detects_address_requests(self, content_filter):
        """Should detect address requests."""
        result = content_filter.filter("ä½ å®¶ä½åœ¨å“ªè£¡")
        assert ContentCategory.PERSONAL_INFO_REQUEST in result.categories_detected

    def test_detects_name_requests(self, content_filter):
        """Should detect full name requests."""
        result = content_filter.filter("ä½ çš„çœŸåæ˜¯ä»€éº¼")
        assert ContentCategory.PERSONAL_INFO_REQUEST in result.categories_detected


class TestContentFilterOffTopic:
    """Tests for off-topic content detection."""

    @pytest.fixture
    def content_filter(self):
        """Create a Content Filter instance."""
        return ContentFilter()

    def test_detects_completely_off_topic(self, content_filter):
        """Should detect completely off-topic content."""
        result = content_filter.filter(
            "ä»Šå¤©çš„è‚¡ç¥¨å¸‚å ´æ€éº¼æ¨£", context={"story_title": "å°å…”å­å†’éšªè¨˜"}
        )
        assert ContentCategory.OFF_TOPIC in result.categories_detected

    def test_allows_related_educational_tangent(self, content_filter):
        """Should allow related educational tangents."""
        result = content_filter.filter(
            "å…”å­çœŸçš„åªåƒç´…è˜¿è””å—", context={"story_title": "å°å…”å­å†’éšªè¨˜"}
        )
        # Related to story animal, should be allowed
        assert result.is_safe is True


class TestContentFilterFearInducing:
    """Tests for fear-inducing content filtering."""

    @pytest.fixture
    def content_filter(self):
        """Create a Content Filter instance."""
        return ContentFilter()

    def test_moderates_scary_content(self, content_filter):
        """Should moderate excessively scary content."""
        result = content_filter.filter("å¯æ€•çš„æ€ªç‰©æœƒåœ¨æ™šä¸Šä¾†æŠ“ä½ ")
        assert (
            result.is_safe is False or ContentCategory.FEAR_INDUCING in result.categories_detected
        )

    def test_allows_mild_story_tension(self, content_filter):
        """Should allow mild story tension appropriate for children."""
        result = content_filter.filter(
            "å°å…”å­æœ‰é»ç·Šå¼µï¼Œå› ç‚ºæ£®æ—è£¡æœ‰å¥‡æ€ªçš„è²éŸ³", context={"story_title": "å°å…”å­å†’éšªè¨˜"}
        )
        assert result.is_safe is True


class TestContentFilterResponseValidation:
    """Tests for validating AI response content."""

    @pytest.fixture
    def content_filter(self):
        """Create a Content Filter instance."""
        return ContentFilter()

    def test_validates_ai_response_safety(self, content_filter):
        """Should validate AI responses for child safety."""
        ai_response = "å°å…”å­é–‹å¿ƒåœ°è·³åˆ°èƒ¡è˜¿è””åœ’ï¼Œæ‰¾åˆ°äº†æœ€å¤§æœ€ç”œçš„èƒ¡è˜¿è””ï¼"
        result = content_filter.is_safe(ai_response)
        assert result is True

    def test_rejects_unsafe_ai_response(self, content_filter):
        """Should reject unsafe AI responses."""
        ai_response = "ä¸é©åˆå…’ç«¥çš„å…§å®¹"
        result = content_filter.is_safe(ai_response)
        # Should be checked and potentially rejected
        assert result is True or result is False  # Depends on actual content

    def test_filter_and_modify_response(self, content_filter):
        """Should filter and potentially modify responses."""
        ai_response = "é€™æ˜¯å›æ‡‰ï¼Œä½†æœ‰ä¸€äº›[å•é¡Œå…§å®¹]éœ€è¦ç§»é™¤"
        filtered = content_filter.filter_response(ai_response)
        # Should return modified or original text
        assert filtered is not None


class TestContentFilterEdgeCases:
    """Edge case tests for Content Filter."""

    @pytest.fixture
    def content_filter(self):
        """Create a Content Filter instance."""
        return ContentFilter()

    def test_handles_empty_input(self, content_filter):
        """Should handle empty input gracefully."""
        result = content_filter.filter("")
        assert result.is_safe is True

    def test_handles_whitespace_only(self, content_filter):
        """Should handle whitespace-only input."""
        result = content_filter.filter("   ")
        assert result.is_safe is True

    def test_handles_very_long_input(self, content_filter):
        """Should handle very long input."""
        long_text = "å°å…”å­" * 1000
        result = content_filter.filter(long_text)
        assert result is not None

    def test_handles_mixed_languages(self, content_filter):
        """Should handle mixed language input."""
        result = content_filter.filter("å°å…”å­ says Hello åœ¨ forest")
        assert result is not None

    def test_handles_emojis(self, content_filter):
        """Should handle emoji content."""
        result = content_filter.filter("å°å…”å­å¥½é–‹å¿ƒ ğŸ°âœ¨")
        assert result.is_safe is True

    def test_handles_special_characters(self, content_filter):
        """Should handle special characters."""
        result = content_filter.filter("å°å…”å­èªªï¼šã€Œä½ å¥½ï¼ã€")
        assert result.is_safe is True


class TestContentFilterConfidence:
    """Tests for content filter confidence scoring."""

    @pytest.fixture
    def content_filter(self):
        """Create a Content Filter instance."""
        return ContentFilter()

    def test_high_confidence_safe(self, content_filter):
        """Should have high confidence for clearly safe content."""
        result = content_filter.filter("ä½ å¥½ï¼")
        assert result.confidence >= 0.9

    def test_low_confidence_ambiguous(self, content_filter):
        """Should have lower confidence for ambiguous content."""
        result = content_filter.filter("é€™å€‹æœ‰é»å¥‡æ€ªçš„å…§å®¹")
        # Ambiguous content should have lower confidence
        assert result.confidence is not None

    def test_confidence_affects_decision(self, content_filter):
        """Low confidence should flag for review."""
        result = content_filter.filter("æ¨¡ç³Šçš„å…§å®¹éœ€è¦å¯©æŸ¥")
        # Should either be flagged or have confidence score
        assert hasattr(result, "confidence") or hasattr(result, "needs_review")
